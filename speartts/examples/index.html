<!DOCTYPE html>
<html>
  <head>
    <title>SPEAR-TTS</title>
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css"
      rel="stylesheet"
    />
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
    <script src="helper.js" defer></script>
    <style>
      td {
        vertical-align: middle;
      }
      audio {
        width: 20vw;
        min-width: 100px;
        max-width: 250px;
      }
    </style>
  </head>
  <body>
    <div class="container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded">
      <div class="text-center">
        <h1>Speak, Read and Prompt:</h1><h3>High-Fidelity Text-to-Speech with Minimal Supervision</h3>
        <p class="lead fw-bold">
          |<a
            href="https://arxiv.org/abs/2302.03540"
            class="btn border-white bg-white fw-bold"
            >paper</a
          >|
        </p>
        <p class="fst-italic mb-0">
          Eugene Kharitonov, Damien Vincent,  Zalán Borsos, Raphaël Marinier, Sertan Girgin, Olivier Pietquin,
          Matt Sharifi, Marco Tagliasacchi, Neil Zeghidour
        </p>
        <p><b>Google Research</b></p>
      </div>
      <p>
        <b>Abstract.</b>  
        We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to "reading") and from semantic tokens to low-level acoustic tokens ("speaking"). Decoupling these two tasks enables training of the "speaking" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the "reading" component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker-id labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in terms of naturalness and acoustic quality, as measured in subjective tests.


      </p>
    </div>

<div class="container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded">
    <h2 id="model-overview" style="text-align: center;">SPEAR-TTS Overview</h2>
    <body>
    <p style="text-align: center;">
        <img src="spear_arch.png" height="200" width="800" class="img-fluid">
    </p>
    </body>
        <p>
SPEAR-TTS operates in two stages, each solving a sequence-to-sequence task. The first stage ("reading") maps the input text transcript to a sequence of semantic tokens. The second stage ("speaking") converts semantic tokens to acoustic tokens. Acoustic tokens are decoded to audio waveforms using a SoundStream decoder (Zeghidour et al., 2021). With this setup, only the first stage requires parallel text-audio data during training, while the second stage can be trained using abundant audio-only data. By combining audio-only pretraining and backtranslation when training the first stage, we reduce the amount of parallel data required to train a high-fidelity TTS system to as little as 15 minutes.
        </p>
</div>

<div class="container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded">
    <h2 id="model-overview" style="text-align: left;">Table of Contents</h2>
    <body>
    <p style="text-align: left;">
    <ul style="list-style: outside none none !important;">
       <li><a href="#efficiency" class="btn border-white bg-white fw-bold">Supervision Efficiency</a></li>
       <li><a href="#diversity" class="btn border-white bg-white fw-bold">Speech Diversity</a></li>
       <li><a href="#prompting" class="btn border-white bg-white fw-bold">Voice Prompting</a></li>
       <li><a href="#impact" class="btn border-white bg-white fw-bold">Broader Impact</a></li>
    </ul>
    </p>
    </body>
</div>


    <div class="container shadow p-5 mb-5 bg-white rounded">
      <h3>Supervision Efficiency<a id="efficiency"/></h3>
      <p class="mb-0">
In the first set of samples, we demonstrate that SPEAR-TTS synthesizes speech that adheres to the input text even when trained on very limited parallel data.
      </p>

      <p style="margin-top: 2em">
In the following table, we provide speech samples produced by SPEAR-TTS trained on a 15 min subsample of LJSpeech (Ito and Johnson, 2017). As inputs, we consider transcripts from LibriTTS test-clean examples (Zen et al., 2019). The models did not see any of these examples during training. In this experiment, we do not prompt our model to follow any specific voice. Note that SPEAR-TTS generates speech in multiple voices, even though it is trained on the single-speaker parallel data. As a reference baseline, we also provide speech samples produced by a modification of FastSpeech2 (Ren et al, 2020), adapted for the low-resource scenario (Pine, 2022). We refer to it as FastSpeech2-LR. These models are trained on subsets of LJSpeech.
              </p>
      <div class="container pt-3 table-responsive">
        <table
          class="table table-hover"
          id="supervision-efficiency-table"
        >
          <thead>
            <tr>
              <th style="text-align: center">Text</th>
              <th style="text-align: center">Ground-truth</th>
              <th style="text-align: center">FastSpeech2-LR (24 h)</th>
              <th style="text-align: center">FastSpeech2-LR (15 min)</th>
              <th style="text-align: center">SPEAR-TTS (15 min)</th>
            </tr>
          </thead>
          <tbody>
             <!-- <tr>
               <td colspan="4" style="text-align: center">LJSpeech (test set)</td>
             </tr>
             <tr height=200px> <td></td> <td></td> <td></td> <td></td> </tr>
             <tr height=200px> <td></td> <td></td> <td></td> <td></td> </tr>
             <tr height=200px> <td></td> <td></td> <td></td> <td></td> </tr>
            -->
             <!-- <tr>
               <td colspan="4" style="text-align: center">LibriSpeech (test-clean)</td>
             </tr> -->
             <tr height=200px> <td></td> <td></td> <td></td> <td></td> <td></td></tr>
             <tr height=200px> <td></td> <td></td> <td></td> <td></td> <td></td></tr>
             <tr height=200px> <td></td> <td></td> <td></td> <td></td> <td></td></tr>
             <tr height=200px> <td></td> <td></td> <td></td> <td></td> <td></td></tr>
             <tr height=200px> <td></td> <td></td> <td></td> <td></td> <td></td></tr>
             <tr height=200px> <td></td> <td></td> <td></td> <td></td> <td></td></tr>
             <tr height=200px> <td></td> <td></td> <td></td> <td></td> <td></td></tr>
             <tr height=200px> <td></td> <td></td> <td></td> <td></td> <td></td></tr>
             <tr height=200px> <td></td> <td></td> <td></td> <td></td> <td></td></tr>
          </tbody>
        </table>
      </div>

      <p style="margin-top: 2em">
      In the next experiment, we show how fidelity to the text changes for various training scenarios, as we vary the amount of available parallel data. '-' indicates that the speech synthesized by the corresponding model is not intelligible.

      </p>
      <div class="container pt-3 table-responsive">
        <table
          class="table table-hover"
          id="fidelity-vs-amount-paired-data"
        >
          <thead>
            <tr>
              <th width="25%">Amount of paired data</th>
              <th width="25%">From scratch</th>
              <th width="25%">+ Pretraining</th>
              <th width="25%">+ Backtranslation</th>
            </tr>
          </thead>
          <tbody>
             <tr>
               <td colspan="4" style="text-align: center">Text: <font size="-1"> Whoever, therefore, is ambitious of distinction in this way ought to be prepared for disappointment. </font></td>
             </tr>
             <tr> <td>24h</td> <td></td> <td></td> <td></td> </tr>
             <tr> <td>12h</td> <td></td> <td></td> <td></td> </tr>
             <tr> <td>3h</td> <td></td> <td></td> <td></td> </tr>
             <tr> <td>2h</td> <td></td> <td></td> <td></td> </tr>
             <tr> <td>1h</td> <td></td> <td></td> <td></td> </tr>
             <tr> <td>30min</td> <td>-</td> <td></td> <td></td> </tr>
             <tr> <td>15min</td> <td>-</td> <td>-</td> <td></td> </tr>
             <tr>
              <td colspan="4" style="text-align: center">Text: <font size="-1"> Suddenly, for the purpose of restoring peace and order, Spring, accompanied by his whole court, made his appearance. </font></td>
             </tr>
             <tr> <td>24h</td> <td></td> <td></td> <td></td> </tr>
             <tr> <td>12h</td> <td></td> <td></td> <td></td> </tr>
             <tr> <td>3h</td> <td></td> <td></td> <td></td> </tr>
             <tr> <td>2h</td> <td></td> <td></td> <td></td> </tr>
             <tr> <td>1h</td> <td></td> <td></td> <td></td> </tr>
             <tr> <td>30min</td> <td>-</td> <td></td> <td></td> </tr>
             <tr> <td>15min</td> <td>-</td> <td>-</td> <td></td> </tr>

          </tbody>
        </table>
      </div>
    </div>

    <div class="container shadow p-5 mb-5 bg-white rounded">
      <h3>Speech diversity<a id="diversity"/></h3>
      <p class="mb-0">
      In this experiment we show that, for a fixed text input, SPEAR-TTS is able to generate diverse speech that varies in terms of prosody and voice characteristics. We use the SPEAR-TTS model trained on a 15 minute subset of LJSpeech (single-speaker) as parallel data. We use transcripts from LibriTTS test-clean (Zen et al., 2019).
      </p>
      <div class="container pt-3 table-responsive">
        <table
          class="table table-hover"
          id="speech-diversity"
        >
          <thead>
            <tr>
              <th width="20%">Text</th>
              <th width="20%">Sample #1</th>
              <th width="20%">Sample #2</th>
              <th width="20%">Sample #3</th>
              <th width="20%">Sample #4</th>
            </tr>
          </thead>
          <tbody>
             <tr> <td></td> <td></td> <td></td> <td></td> <td></td> </tr>
             <tr> <td></td> <td></td> <td></td> <td></td> <td></td> </tr>
             <tr> <td></td> <td></td> <td></td> <td></td> <td></td> </tr>
          </tbody>
        </table>
      </div>
    </div>

    <div class="container shadow p-5 mb-5 bg-white rounded">
      <h3>Voice Prompting<a id="prompting"/></h3>
      <p class="mb-0">
      In the following set of samples we show that, when prompted with a 3-second sample, SPEAR-TTS generates an utterance that is faithful to the prompted voice. Here, voices are selected from LibriSpeech test-clean (Panayotov et al., 2015), so the model has never seen them during training, thus demonstrating zero-shot generalization. We use the SPEAR-TTS model trained on a 15 minute subset of LJSpeech (single-speaker) as parallel data.
      </p>
      <div class="container pt-3 table-responsive">
        <table
          class="table table-hover"
          id="prompting-table"
        >
          <thead>
            <tr>
              <th width="20%">Prompt</th>
              <th width="20%">Sample #1</th>
              <th width="20%">Sample #2</th>
              <th width="20%">Sample #3</th>
              <th width="20%">Sample #4</th>
            </tr>
          </thead>
          <tbody>
             <tr>
               <td colspan="5" style="text-align: center">Text: <font size="-1">
                   "And yesterday things went on just as usual"
               </font></td>
             </tr>
             <tr> <td>Speaker #1 Prompt #1</td> <td></td> <td></td> <td></td> <td></td> </tr>
             <tr> <td>Speaker #1 Prompt #2</td> <td></td> <td></td> <td></td> <td></td> </tr>
             <tr> <td>Speaker #2 Prompt #1</td> <td></td> <td></td> <td></td> <td></td> </tr>
             <tr> <td>Speaker #2 Prompt #2</td> <td></td> <td></td> <td></td> <td></td> </tr>
             <tr>
               <td colspan="5" style="text-align: center">Text: <font size="-1">
                   "I see a crowd in one corner of the garden everybody standing still and looking up"
               </font></td>
             </tr>
             <tr> <td>Speaker #1 Prompt #1</td> <td></td> <td></td> <td></td> <td></td> </tr>
             <tr> <td>Speaker #1 Prompt #2</td> <td></td> <td></td> <td></td> <td></td> </tr>
             <tr> <td>Speaker #2 Prompt #1</td> <td></td> <td></td> <td></td> <td></td> </tr>
             <tr> <td>Speaker #2 Prompt #2</td> <td></td> <td></td> <td></td> <td></td> </tr>
          </tbody>
        </table>
      </div>
    </div>


    <div class="container shadow p-5 mb-5 bg-white rounded">
      <h3>Broader impact<a id="impact"/></h3>
      <p class="mb-0">
      We believe our work on high-quality TTS that requires very limited supervision (quantity- and quality-wise) can be an important stepping stone for enabling this core speech technology for communities that are currently underserved by TTS solutions due to speaking "low-resource" languages, i.e., languages do not have vast parallel corpora required for training deep learning models. Even for high-resource languages, such as English, the ability to harness unpaired data for speech generation can enable producing speech in accents and dialects that are currently uncovered by the existing TTS systems.
      </p>
      <br />
      <p>
      At the same time, we acknowledge that the ability to mimic a voice can have numerous malicious applications, including bypassing biometric identification for the purpose of impersonation. Thus it is crucial to put safeguards against the misuse of SPEAR-TTS and, as an initial step, we verify that speech produced by SPEAR-TTS can be reliably detected by a classifier (see Appendix E). In the future, one can explore other approaches for detecting synthesized speech, for example by audio watermarking.
     </p>
    </div>



  </body>
</html>
