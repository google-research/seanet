<!DOCTYPE html>
<html>
  <head>
    <title>AudioLM</title>
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css"
      rel="stylesheet"
    />
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
    <script src="helper.js" defer></script>
    <style>
      td {
        vertical-align: middle;
      }
      audio {
        width: 20vw;
        min-width: 100px;
        max-width: 250px;
      }
    </style>
  </head>
  <body>
    <div class="container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded">
      <div class="text-center">
        <h1>AudioLM</h1>
        <h3>A Language Modeling Approach to Audio Generation</h3>
        <p class="lead fw-bold">
          |<a
            href="https://arxiv.org/abs/2209.03143"
            class="btn border-white bg-white fw-bold"
            >paper</a
          >|
          <a
            href="https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html"
            class="btn border-white bg-white fw-bold"
            >blog post</a
          >|
        </p>
        <p class="fst-italic mb-0">
          Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov,
          Olivier Pietquin, Matt Sharifi, Olivier Teboul, David Grangier, Marco
          Tagliasacchi, Neil Zeghidour
        </p>
        <p>Google Research</p>
      </div>
      <p>
        <b>Abstract.</b> We introduce AudioLM, a framework for high-quality
        audio generation with long-term consistency. AudioLM maps the input
        audio to a sequence of discrete tokens and casts audio generation as a
        language modeling task in this representation space. We show how
        existing audio tokenizers provide different trade-offs between
        reconstruction quality and long-term structure, and we propose a hybrid
        tokenization scheme to achieve both objectives. Namely, we leverage the
        discretized activations of a masked language model pre-trained on audio
        to capture long-term structure and the discrete codes produced by a
        neural audio codec to achieve high-quality synthesis. By training on
        large corpora of raw audio waveforms, AudioLM learns to generate natural
        and coherent continuations given short prompts. When trained on speech,
        and without any transcript or annotation, AudioLM generates
        syntactically and semantically plausible speech continuations while also
        maintaining speaker identity and prosody for unseen speakers.
        Furthermore, we demonstrate how our approach extends beyond speech by
        generating coherent piano music continuations, despite being trained
        without any symbolic representation of music.
      </p>
    </div>

    <div class="container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded">
      <h3>Speech continuation</h3>
      <p class="mb-0">
        <br />
        Continuations using 3 second prompts from LibriSpeech test-{clean,
        other}, for speakers and content not seen during training. AudioLM
        excels at generating continuations that:
      </p>
      <ul>
        <li>
          preserve speaker identity, prosody, accent and recording conditions of
          the prompt,
        </li>
        <li>have syntactically correct and semantically coherent content.</li>
      </ul>

      <div class="container pt-3">
        <h5>Librispeech test-clean</h5>

        <div class="table-responsive pt-3">
          <table
            class="table table-striped table-hover pt-2"
            id="librispeech-test-clean-table"
          >
            <thead>
              <tr>
                <th>Original</th>
                <th>Prompt</th>
                <th>Continuations</th>
              </tr>
            </thead>
            <tbody></tbody>
          </table>
          <ul class="pagination justify-content-center">
            <li class="page-item active">
              <a id="clean-cont-page-1" class="page-link" href="#">1</a>
            </li>
            <li class="page-item">
              <a id="clean-cont-page-2" class="page-link" href="#">2</a>
            </li>
            <li class="page-item">
              <a id="clean-cont-page-3" class="page-link" href="#">3</a>
            </li>
            <li class="page-item">
              <a id="clean-cont-page-4" class="page-link" href="#">4</a>
            </li>
            <li class="page-item">
              <a id="clean-cont-page-5" class="page-link" href="#">5</a>
            </li>
          </ul>
        </div>
      </div>

      <div class="container pt-5">
        <h5>Librispeech test-other</h5>
        <div class="table-responsive pt-3">
          <table
            class="table table-striped table-hover pt-2"
            id="librispeech-test-other-table"
          >
            <thead>
              <tr>
                <th>Original</th>
                <th>Prompt</th>
                <th>Continuations</th>
              </tr>
            </thead>
            <tbody></tbody>
          </table>
          <ul class="pagination justify-content-center">
            <li class="page-item active">
              <a id="other-cont-page-1" class="page-link" href="#">1</a>
            </li>
            <li class="page-item">
              <a id="other-cont-page-2" class="page-link" href="#">2</a>
            </li>
          </ul>
        </div>
      </div>
    </div>

    <div class="container shadow p-5 mb-5 bg-white rounded">
      <h3>Acoustic generation</h3>

      <p class="mb-0">
        <br />
        For acoustic generation, we sample the acoustic tokens given the
        semantic tokens extracted from the original samples from LibriSpeech
        test-clean. The model generates samples with different speakers and
        recording conditions, while the semantic content is identical.
      </p>

      <div class="container pt-3 table-responsive">
        <table
          class="table table-striped table-hover"
          id="acoustic-generation-table"
        >
          <thead>
            <tr>
              <th>Original</th>
              <th>Acoustic generation (stage 2 and 3)</th>
            </tr>
          </thead>
          <tbody></tbody>
        </table>
        <ul class="pagination justify-content-center">
          <li class="page-item active">
            <a id="acoustic-page-1" class="page-link" href="#">1</a>
          </li>
          <li class="page-item">
            <a id="acoustic-page-2" class="page-link" href="#">2</a>
          </li>
          <li class="page-item">
            <a id="acoustic-page-3" class="page-link" href="#">3</a>
          </li>
          <li class="page-item">
            <a id="acoustic-page-4" class="page-link" href="#">4</a>
          </li>
          <li class="page-item">
            <a id="acoustic-page-5" class="page-link" href="#">5</a>
          </li>
        </ul>
      </div>
    </div>

    <div class="container shadow p-5 mb-5 bg-white rounded">
      <h3>Unconditional generation</h3>

      <p class="mb-0">
        <br />
        The unconditional generation performs sampling without using prompts. In
        that case, every sequence varies in speaker identity, linguistic
        content, and recording conditions.
      </p>

      <div class="container pt-3 table-responsive">
        <table
          class="table table-striped table-hover"
          id="unconditional-speech-table"
        >
          <thead>
            <tr>
              <th>Samples from unconditional generation</th>
            </tr>
          </thead>
          <tbody></tbody>
        </table>
      </div>
    </div>

    <div class="container shadow p-5 mb-5 bg-white rounded">
      <h3>Generation without semantic tokens</h3>

      <p class="mb-0">
        <br />
        To illustrate that the semantic tokens are crucial for generating
        coherent linguistic content, we train the language model on the acoustic
        tokens only. While the generated continuations of the 4-second prompts
        maintain speaker identity, the linguistic content is inconsistent, and
        often akin to babbling.
      </p>

      <div class="container pt-3 table-responsive">
        <table class="table table-striped table-hover" id="semantic-only-table">
          <thead>
            <tr>
              <th>
                Continuations with a language model trained on the acoustic
                tokens only (without semantic tokens)
              </th>
            </tr>
          </thead>
          <tbody></tbody>
        </table>
      </div>
    </div>

    <div class="container shadow p-5 mb-5 bg-white rounded">
      <h3>Comparing SoundStream reconstructions</h3>
      <p class="mb-0">
        <br />
        We compare SoundStream reconstructions of two models, one using 3-layer
        residual vector quantization (3-RVQ) and another with 12 layers
        (12-RVQ), the latter being the default. The equivalent bitrates are 1.5
        kbps and 6 kbps.
      </p>
      <div class="container pt-3 table-responsive">
        <table
          class="table pt-2 table-striped table-hover"
          id="soundstream-table"
        >
          <thead>
            <tr>
              <th>Original</th>
              <th>SoundStream 3-RVQ</th>
              <th>SoundStream 12-RVQ</th>
            </tr>
          </thead>
          <tbody></tbody>
        </table>
        <ul class="pagination justify-content-center">
          <li class="page-item active">
            <a id="soundstream-page-1" class="page-link" href="#">1</a>
          </li>
          <li class="page-item">
            <a id="soundstream-page-2" class="page-link" href="#">2</a>
          </li>
        </ul>
      </div>
    </div>

    <div class="container shadow p-5 mb-5 bg-white rounded">
      <h3>Piano continuation</h3>
      <p class="mb-0">
        <br />
        AudioLM is not limited to modeling speech. It can also learn to generate
        coherent piano music continuations, despite being trained on piano music
        without any symbolic representation. We also show the continuations
        produced by a version of the model trained exclusively on the acoustic
        tokens. These continuations are much less coherent, stressing the
        importance of the semantic tokens in our framework. The 4-second prompts
        come from the test split of
        <a
          href="https://magenta.tensorflow.org/datasets/maestro"
          class="text-decoration-none"
          >MAESTRO</a
        >
        dataset.
      </p>
      <div class="container pt-3 table-responsive">
        <table class="table pt-2 table-striped table-hover" id="piano-table">
          <thead>
            <tr>
              <th>Original</th>
              <th>Prompt</th>
              <th>Continuation by acoustic-only model</th>
              <th>Continuation by AudioLM</th>
            </tr>
          </thead>
          <tbody></tbody>
        </table>
      </div>
    </div>
  </body>
</html>
